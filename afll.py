import ply.lex as lex
import ply.yacc as yacc

# ----- Lexer -----
# Token definitions
tokens = [
    'B_TAG', 'I_TAG', 'CLOSE_B_TAG', 'CLOSE_I_TAG',
    'TEXT'
]

# Regular expressions for tokens
t_B_TAG = r'<b>'
t_I_TAG = r'<i>'
t_CLOSE_B_TAG = r'</b>'
t_CLOSE_I_TAG = r'</i>'
t_TEXT = r'[^<]+'

# Error handling rule for the lexer
def t_error(t):
    print(f"Illegal character '{t.value[0]}' at position {t.lexpos}")
    t.lexer.skip(1)

# Build the lexer
lexer = lex.lex()

# ----- Parser -----
# Define grammar rules
def p_html_content(p):
    '''html_content : element html_content
                    | element
                    | TEXT html_content
                    | TEXT'''
    if len(p) == 3:
        p[0] = [p[1]] + p[2]
    else:
        p[0] = [p[1]]

def p_element(p):
    '''element : bold
                | italics'''
    p[0] = p[1]

def p_bold(p):
    '''bold : B_TAG html_content CLOSE_B_TAG
            | B_TAG CLOSE_B_TAG'''
    if len(p) == 4:
        p[0] = ('bold', p[2])
    else:
        p[0] = ('bold', [])

def p_italics(p):
    '''italics : I_TAG html_content CLOSE_I_TAG
                | I_TAG CLOSE_I_TAG'''
    if len(p) == 4:
        p[0] = ('italics', p[2])
    else:
        p[0] = ('italics', [])

# Error handling rule for the parser
def p_error(p):
    if p:
        print(f"Syntax error at token {p.type}: '{p.value}'")
    else:
        print("Syntax error at EOF")

# Build the parser
parser = yacc.yacc()

# ----- Test Cases -----
# Get test cases from user
test_cases = []
num_cases = int(input("Enter number of test cases: "))
for i in range(num_cases):
    test_cases.append(input(f"Enter test case {i + 1}: "))

# Run the lexer and parser on each test case
for case in test_cases:
    print(f"\nRunning lexer and parser on test case: {case}")

    # Input the data to the lexer
    lexer.input(case)

    # Tokenize and print tokens
    token_stack = []
    for token in lexer:
        token_stack.append(token)
        print(token) # Print each token generated by the lexer

    
    # Parse the test case and display results
    result = parser.parse(case, lexer=lexer)

# Display the structure of parsed HTML content
if result:
    print("Parsed structure:")
    for element in result:
        print(f" - {element}")
